{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####GraphRAG\n",
    "import os\n",
    "import networkx as nx\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chardet\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "os.chdir(\"/Users/kdk/Desktop/2024/project/storyteller/api\")\n",
    "load_dotenv()\n",
    "\n",
    "from text_util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    return result['encoding']\n",
    "\n",
    "def load_file(file_path):\n",
    "    try:\n",
    "        encoding = detect_encoding(file_path)\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "class GraphRAGChatbot:\n",
    "    def __init__(self, openai_api_key):\n",
    "        self.embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        self.chat_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7, openai_api_key=openai_api_key)\n",
    "        self.topics = ['economics', 'science', 'law', 'social', 'environment', 'education', 'politics', 'culture']\n",
    "        self.persona_graphs = {}\n",
    "        self.build_graphs()\n",
    "\n",
    "    def build_graphs(self):\n",
    "        \"\"\"Build graphs for each topic by reading txt files and creating connections.\"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=50,\n",
    "        )\n",
    "        base_dir = '/content/drive/MyDrive/ai papers/new2'\n",
    "\n",
    "        for topic in self.topics:\n",
    "            folder_path = os.path.join(base_dir, topic)\n",
    "            documents = []\n",
    "            if os.path.exists(folder_path):\n",
    "                for filename in os.listdir(folder_path):\n",
    "                    if filename.endswith('.txt'):\n",
    "                        file_path = os.path.join(folder_path, filename)\n",
    "                        text = load_file(file_path)\n",
    "                        if text:\n",
    "                            doc = Document(page_content=text, metadata={\"source\": file_path})\n",
    "                            docs = text_splitter.split_documents([doc])\n",
    "                            documents.extend(docs)\n",
    "\n",
    "                graph = nx.Graph()\n",
    "                for i, doc in enumerate(documents):\n",
    "                    graph.add_node(i, content=doc.page_content, metadata=doc.metadata)\n",
    "\n",
    "                embeddings =[self.embedding_model.embed_query(doc.page_content) for doc in documents]\n",
    "                norms = [np.linalg.norm(embedding) for embedding in embeddings]\n",
    "                print(\"construct graph\", topic)\n",
    "                for i in range(len(documents)):\n",
    "                    for j in range(i + 1, len(documents)):\n",
    "                        similarity = np.dot(embeddings[i], embeddings[j]) / (norms[i] * norms[j])\n",
    "                        if similarity > 0.5:  # Threshold for similarity\n",
    "                            graph.add_edge(i, j, weight=similarity)\n",
    "\n",
    "                self.persona_graphs[topic] = graph\n",
    "            else:\n",
    "                print(f\"Folder for topic '{topic}' does not exist.\")\n",
    "\n",
    "    def compute_similarity(self, text1, text2):\n",
    "        \"\"\"Compute semantic similarity using embeddings.\"\"\"\n",
    "        embedding1 = self.embedding_model.embed_query(text1)\n",
    "        embedding2 = self.embedding_model.embed_query(text2)\n",
    "        return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "    def recommend_topics(self, query):\n",
    "        \"\"\"Use OpenAI API to recommend 5 most related topics based on the query.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the query: '{query}', select the 5 most relevant topics from the following list:\\n\"\n",
    "            f\"{', '.join(self.topics)}\\n\"\n",
    "            f\"Provide only the list of topics separated by commas.\"\n",
    "        )\n",
    "        response = self.chat_model([HumanMessage(content=prompt)])\n",
    "        recommended_topics = [topic.strip() for topic in response.content.split(',')]\n",
    "        recommended_topics = [topic for topic in recommended_topics if topic in self.topics]\n",
    "        return recommended_topics[:5]\n",
    "\n",
    "    def answer_question(self, query, personas):\n",
    "        \"\"\"Answer questions using GraphRAG.\"\"\"\n",
    "        answers = {}\n",
    "        for persona in personas:\n",
    "            if persona in self.persona_graphs:\n",
    "                graph = self.persona_graphs[persona]\n",
    "                # Find relevant nodes based on the query\n",
    "                relevant_nodes = self.retrieve_relevant_nodes(graph, query)\n",
    "\n",
    "                # Aggregate content from relevant nodes\n",
    "                context = \"\\n\\n\".join([graph.nodes[node]['content'] for node in relevant_nodes])\n",
    "\n",
    "                # Generate answer\n",
    "                prompt_template = PromptTemplate(\n",
    "                    input_variables=[\"persona\", \"context\", \"question\"],\n",
    "                    template=(\n",
    "                        \"Please think from a {persona} perspective.\\n\\n\"\n",
    "                        \"{context}\\n\\n\"\n",
    "                        \"Question: {question}\\n\"\n",
    "                        \"Answer:\"\n",
    "                    )\n",
    "                )\n",
    "                chain = LLMChain(\n",
    "                    llm=self.chat_model,\n",
    "                    prompt=prompt_template\n",
    "                )\n",
    "                output = chain.run(\n",
    "                    persona=persona,\n",
    "                    context=context,\n",
    "                    question=query\n",
    "                )\n",
    "                answers[persona] = output\n",
    "            else:\n",
    "                answers[persona] = f\"No data available for persona '{persona}'.\"\n",
    "        return answers\n",
    "\n",
    "    def retrieve_relevant_nodes(self, graph, query):\n",
    "        \"\"\"Retrieve nodes from the graph that are most relevant to the query.\"\"\"\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "        similarities = {}\n",
    "        for node, data in graph.nodes(data=True):\n",
    "            node_embedding = self.embedding_model.embed_query(data['content'])\n",
    "            similarity = np.dot(query_embedding, node_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(node_embedding))\n",
    "            similarities[node] = similarity\n",
    "\n",
    "        # Sort nodes by similarity and return the top N\n",
    "        sorted_nodes = sorted(similarities, key=similarities.get, reverse=True)\n",
    "        return sorted_nodes[:5]\n",
    "\n",
    "    def debate_question(self, question, personas):\n",
    "        \"\"\"Simulate a debate between personas.\"\"\"\n",
    "        from langchain.prompts import PromptTemplate\n",
    "        from langchain.chains import LLMChain\n",
    "\n",
    "        # Retrieve context for each persona\n",
    "        persona_contexts = {}\n",
    "        for persona in personas:\n",
    "            if persona in self.persona_graphs:\n",
    "                #retriever = self.persona_graphs[persona].as_retriever()\n",
    "                graph = self.persona_graphs[persona]\n",
    "                #docs = retriever.get_relevant_documents(question)\n",
    "                relevant_nodes = self.retrieve_relevant_nodes(graph, question)\n",
    "                context = \"\\n\\n\".join([graph.nodes[node]['content'] for node in relevant_nodes])\n",
    "                #context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "                persona_contexts[persona] = context\n",
    "\n",
    "            else:\n",
    "                persona_contexts[persona] = \"\"\n",
    "\n",
    "        # Initialize dialogue history\n",
    "        dialogue_history = []\n",
    "        max_turns = 10  # Each persona speaks up to 10 times\n",
    "        total_exchanges = max_turns * len(personas)\n",
    "\n",
    "        # Generate initial answers\n",
    "        for persona in personas:\n",
    "            prompt_template = PromptTemplate(\n",
    "                input_variables=[\"persona\", \"context\", \"question\"],\n",
    "                template=(\n",
    "                    \"As a {persona}, based on the following   :\\n\\n\"\n",
    "                    \"{context}\\n\\n\"\n",
    "                    \"Answer the question: {question}\\n\"\n",
    "                    \"Your answer:\"\n",
    "                )\n",
    "            )\n",
    "            chain = LLMChain(\n",
    "                llm=self.chat_model,\n",
    "                prompt=prompt_template\n",
    "            )\n",
    "            response = chain.run(\n",
    "                persona=persona,\n",
    "                context=persona_contexts[persona],\n",
    "                question=question\n",
    "            )\n",
    "            dialogue_history.append((persona, response))\n",
    "\n",
    "        # Simulate debate\n",
    "        for _ in range(max_turns):\n",
    "            for persona in personas:\n",
    "                # Compile dialogue history\n",
    "                history_text = \"\"\n",
    "                for speaker, utterance in dialogue_history[-6:]:  # Limit to last 6 exchanges\n",
    "                    history_text += f\"{speaker.capitalize()}: {utterance}\\n\"\n",
    "\n",
    "                # Prepare prompt\n",
    "                prompt_template = PromptTemplate(\n",
    "                    input_variables=[\"persona\", \"context\", \"dialogue_history\", \"question\"],\n",
    "                    template=(\n",
    "                        \"As a {persona}, continue the following debate based on the context and previous dialogue.\\n\\n\"\n",
    "                        \"Context:\\n{context}\\n\\n\"\n",
    "                        \"Dialogue history:\\n{dialogue_history}\\n\"\n",
    "                        \"Question: {question}\\n. Answer within 3 sentences.\"\n",
    "                        \"{persona}, your response:\"\n",
    "                    )\n",
    "                )\n",
    "                chain = LLMChain(\n",
    "                    llm=self.chat_model,\n",
    "                    prompt=prompt_template\n",
    "                )\n",
    "                response = chain.run(\n",
    "                    persona=persona,\n",
    "                    context=persona_contexts[persona],\n",
    "                    dialogue_history=history_text,\n",
    "                    question=question\n",
    "                )\n",
    "                dialogue_history.append((persona, response))\n",
    "\n",
    "                # Check if total exchanges reached 30\n",
    "                if len(dialogue_history) >= total_exchanges:\n",
    "                    break\n",
    "            if len(dialogue_history) >= total_exchanges:\n",
    "                break\n",
    "\n",
    "        # Display the dialogue\n",
    "        print(\"\\nDebate Transcript:\\n\")\n",
    "        for speaker, utterance in dialogue_history:\n",
    "            print(f\"{speaker.capitalize()}: {utterance}\\n\")\n",
    "    def recommend_personas(self, query, selected_topics):\n",
    "        \"\"\"Use LLM to recommend 3 most appropriate personas among selected topics.\"\"\"\n",
    "        prompt = (\n",
    "            f\"Given the query: '{query}' and the topics: {', '.join(selected_topics)}, \"\n",
    "            f\"select the 3 most appropriate topics for answering the question. \"\n",
    "            f\"Provide only the list of topics separated by commas.\"\n",
    "        )\n",
    "        response = self.chat_model([HumanMessage(content=prompt)])\n",
    "        recommended_personas = [topic.strip().lower() for topic in response.content.split(',')]\n",
    "        # Ensure valid topics from selected_topics\n",
    "        recommended_personas = [topic for topic in recommended_personas if topic in selected_topics]\n",
    "        return recommended_personas[:3]\n",
    "\n",
    "    def run_chatbot(self):\n",
    "        \"\"\"Run the chatbot interaction loop.\"\"\"\n",
    "        print(\"Welcome to the GraphRAG Chatbot Service!\")\n",
    "        while True:\n",
    "            print(\"\\n1. Start a new discussion\")\n",
    "            print(\"\\n2. Exit\")\n",
    "            print(\"\\n3. Debate\")\n",
    "\n",
    "            choice = input(\"Select an option: \")\n",
    "\n",
    "            if choice == \"1\":\n",
    "                query = input(\"Enter the topic you want to discuss: \")\n",
    "                recommended_topics = self.recommend_topics(query)\n",
    "                if not recommended_topics:\n",
    "                    print(\"No relevant topics found. Please try again.\")\n",
    "                    continue\n",
    "                print(f\"Recommended topics: {', '.join(recommended_topics)}\")\n",
    "\n",
    "                question = input(\"Enter your question: \")\n",
    "                recommended_personas = self.recommend_personas(question, recommended_topics)\n",
    "                if not recommended_personas:\n",
    "                    print(\"No personas could be recommended based on your question.\")\n",
    "                    continue\n",
    "                print(f\"Personas selected to answer: {', '.join(recommended_personas)}\")\n",
    "\n",
    "                answers = self.answer_question(question, recommended_personas)\n",
    "                for persona, answer in answers.items():\n",
    "                    print(f\"\\nAnswer from '{persona}': {answer}\")\n",
    "\n",
    "            elif choice == \"2\":\n",
    "                print(\"Exiting chatbot. Goodbye!\")\n",
    "                break\n",
    "\n",
    "            elif choice == \"3\":\n",
    "                question = input(\"Enter the question for the debate: \")\n",
    "                recommended_topics = self.recommend_topics(question)\n",
    "                if not recommended_topics:\n",
    "                    print(\"No relevant topics found. Please try again.\")\n",
    "                    continue\n",
    "                print(f\"Recommended topics: {', '.join(recommended_topics)}\")\n",
    "\n",
    "                recommended_personas = self.recommend_personas(question, recommended_topics)\n",
    "                if not recommended_personas:\n",
    "                    print(\"No personas could be recommended based on your question.\")\n",
    "                    continue\n",
    "                print(f\"Personas selected for the debate: {', '.join(recommended_personas)}\")\n",
    "\n",
    "                self.debate_question(question, recommended_personas)\n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai.api_resources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m      3\u001b[0m f \u001b[38;5;241m=\u001b[39m gzip\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./PersonaChatbot_graphRAG_debate_final.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai.api_resources'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "f = gzip.open('./PersonaChatbot_graphRAG_debate_final.pickle','rb')\n",
    "chatbot = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc economics\n",
      "embedding economics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 20/20 [00:25<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph economics\n",
      "doc science\n",
      "embedding science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 57/57 [01:18<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph science\n",
      "doc law\n",
      "embedding law\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 19/19 [00:22<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph law\n",
      "doc social\n",
      "embedding social\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 26/26 [00:41<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph social\n",
      "doc environment\n",
      "embedding environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 58/58 [01:12<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph environment\n",
      "doc education\n",
      "embedding education\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph education\n",
      "doc politics\n",
      "embedding politics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 15/15 [00:17<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph politics\n",
      "doc culture\n",
      "Failed to load ./new2/culture/The Lab’s scientific achievement in.txt: 'charmap' codec can't decode byte 0x9d in position 686: character maps to <undefined>\n",
      "embedding culture\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 19/19 [00:24<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construct graph culture\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'networkx' has no attribute 'write_gpickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m graphs \u001b[38;5;241m=\u001b[39m build_graphs(topics, embedding_model)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics:\n\u001b[0;32m---> 54\u001b[0m     \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_gpickle\u001b[49m(graphs[topic], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./graphrag/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.gpickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'write_gpickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from text_util import *\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "batch_size = 32\n",
    "def build_graphs(topics, embedding_model):\n",
    "    persona_graphs = {}\n",
    "    base_dir = \"./new2\"\n",
    "\n",
    "    for topic in topics:\n",
    "        folder_path = os.path.join(base_dir, topic)\n",
    "        if not os.path.exists(folder_path):\n",
    "            pass\n",
    "        print(\"doc\", topic)\n",
    "        documents = construct_document(folder_path)\n",
    "\n",
    "        graph = nx.Graph()\n",
    "        for i, doc in enumerate(documents):\n",
    "            graph.add_node(i, content=doc.page_content, metadata=doc.metadata)\n",
    "        print(\"embedding\", topic)\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        norms = [np.linalg.norm(embedding) for embedding in embeddings]\n",
    "\n",
    "        print(\"construct graph\", topic)\n",
    "        for i in range(len(documents)):\n",
    "            for j in range(i + 1, len(documents)):\n",
    "                similarity = np.dot(embeddings[i], embeddings[j]) / (\n",
    "                    norms[i] * norms[j]\n",
    "                )\n",
    "                if similarity > 0.5:  # Threshold for similarity\n",
    "                    graph.add_edge(i, j, weight=similarity)\n",
    "\n",
    "        persona_graphs[topic] = graph\n",
    "    return persona_graphs\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "topics = ['economics', 'science', 'law', 'social', 'environment', 'education', 'politics', 'culture']\n",
    "\n",
    "graphs = build_graphs(topics, embedding_model)\n",
    "for topic in topics:\n",
    "    nx.write_gpickle(graphs[topic], f'./graphrag/{topic}.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"./graphrag/grahs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpickle\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import picle\n",
    "\n",
    "pickle.load(open(\"./graphrag/graphs.pkl\", \"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
