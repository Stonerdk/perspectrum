{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "KFE9S-1vv06S",
    "outputId": "bd858a03-816e-4dfc-c21d-da816eb88cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
      "Collecting pypdf2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf2, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1 pypdf2-3.0.1\n",
      "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0.post1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.19)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
      "Collecting configobj (from fitz)\n",
      "  Downloading configobj-5.0.9.tar.gz (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting configparser (from fitz)\n",
      "  Downloading configparser-7.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.3.2)\n",
      "Collecting nipype (from fitz)\n",
      "  Downloading nipype-1.9.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.26.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.2.2)\n",
      "Collecting pyxnat (from fitz)\n",
      "  Downloading pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.13.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.2.0)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (6.4.5)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel->fitz) (4.12.2)\n",
      "Requirement already satisfied: click>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (8.1.7)\n",
      "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.4.2)\n",
      "Collecting prov>=1.5.2 (from nipype->fitz)\n",
      "  Downloading prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (2.8.2)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
      "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting traits>=6.2 (from nipype->fitz)\n",
      "  Downloading traits-6.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.16.1)\n",
      "Collecting acres (from nipype->fitz)\n",
      "  Downloading acres-0.1.0-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting etelemetry>=0.3.1 (from nipype->fitz)\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting looseversion!=1.2 (from nipype->fitz)\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting puremagic (from nipype->fitz)\n",
      "  Downloading puremagic-1.28-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (5.3.0)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (2.32.3)\n",
      "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.3.1->nipype->fitz)\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
      "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->pyxnat->fitz) (2024.8.30)\n",
      "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading configparser-7.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading nipype-1.9.1-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading traits-6.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading acres-0.1.0-py3-none-any.whl (9.1 kB)\n",
      "Downloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: configobj\n",
      "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for configobj: filename=configobj-5.0.9-py2.py3-none-any.whl size=35615 sha256=616610058517b3367371c97ce5a38d724aa02d6e9bcc2e715fad526f4991d660\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/6c/03/6c5e3cf1a6e4b9e2fc5c4409be4abc5a8268bd9c878739cb32\n",
      "Successfully built configobj\n",
      "Installing collected packages: puremagic, looseversion, traits, simplejson, isodate, configparser, configobj, ci-info, acres, rdflib, pyxnat, etelemetry, prov, nipype, fitz\n",
      "Successfully installed acres-0.1.0 ci-info-0.3.0 configobj-5.0.9 configparser-7.1.0 etelemetry-0.3.1 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.3.0 nipype-1.9.1 prov-2.0.1 puremagic-1.28 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.3 traits-6.4.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "70aef08abd6240748d78b8b56692c8e1",
       "pip_warning": {
        "packages": [
         "backports"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.14-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install langchain pypdf2 faiss-cpu sentence-transformers\n",
    "!pip install openai langchain faiss-cpu\n",
    "!pip install fitz\n",
    "!pip install pymupdf\n",
    "!pip install openai==0.28\n",
    "!pip install --upgrade langchain langchain-openai openai\n",
    "!pip install -U langchain-community\n",
    "!pip install pypdf\n",
    "!pip install tiktoken\n",
    "!pip install pdfplumber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pND0y2UR2rok",
    "outputId": "54c6ed5c-0a90-4fff-a075-3ed398a62d31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8K4sDFBvv554",
    "outputId": "d2332282-a72a-4c62-ba43-dcdc8b51e48c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide paths to general knowledge PDFs (uploaded in Colab, comma-separated):\n",
      "General knowledge VectorDB and Knowledge Graph built successfully!\n",
      "Welcome to the Persona Chatbot Service!\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n",
      "Select an option: 1\n",
      "Enter Persona name: sy\n",
      "Enter PDF path for the Persona (uploaded in Colab): /content/drive/MyDrive/ai papers/persona/2006.10726v3 (1).pdf\n",
      "Persona 'sy' added successfully!\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n",
      "Select an option: 2\n",
      "Enter Persona name: sy\n",
      "Enter your question: what is diffusion?\n",
      "general_similarity: 0.8255927750435581\n",
      "persona_similarity: 0.8881068911583554\n",
      "Persona: sy\n",
      "\n",
      "Answer from 'sy': Diffusion is a process where particles spread out from an area of high concentration to an area of low concentration. This movement occurs naturally over time and continues until the concentration of particles is equal in all areas. It plays a significant role in many physical and biological phenomena, such as the spreading of a scent in a room or the movement of oxygen and carbon dioxide between blood and cells in our body.\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n",
      "Select an option: 2\n",
      "Enter Persona name: sy\n",
      "Enter your question: what is stable diffusion model?\n",
      "general_similarity: 0.8426383925292862\n",
      "persona_similarity: 0.8437133072804092\n",
      "Persona: sy\n",
      "\n",
      "Answer from 'sy': The information provided does not contain details about what a \"stable diffusion model\" is.\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n",
      "Select an option: 2\n",
      "Enter Persona name: sy\n",
      "Enter your question: what is latent diffusion model?\n",
      "general_similarity: 0.8494551046930853\n",
      "persona_similarity: 0.8412352411273145\n",
      "General AI Knowledge\n",
      "\n",
      "Answer from 'sy': The provided text does not include information about a latent diffusion model.\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n",
      "Select an option: what is high resolution synthesis with latent diffusion modle?\n",
      "Invalid choice. Please try again.\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n",
      "Select an option: 2\n",
      "Enter Persona name: sy\n",
      "Enter your question: what is high resolution synthesis with latent diffusion model??\n",
      "general_similarity: 0.867157112651923\n",
      "persona_similarity: 0.867157112651923\n",
      "General AI Knowledge\n",
      "\n",
      "Answer from 'sy': As the source provided does not contain specific information about high resolution synthesis with a latent diffusion model, I'm unable to provide a direct definition. However, I can tell you that high resolution synthesis is generally about creating high-quality, detailed digital replications or simulations. A latent diffusion model is likely related to machine learning and AI, specifically in the field of deep learning. In this context, \"latent\" refers to hidden or not directly observable variables, and \"diffusion\" might refer to a type of stochastic process used in modeling. However, without specific context or additional information, these are just educated guesses.\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n",
      "Select an option: 2\n",
      "Enter Persona name: sy\n",
      "Enter your question: what is diffusion model?\n",
      "general_similarity: 0.8408336653474119\n",
      "persona_similarity: 0.8457346590315914\n",
      "Persona: sy\n",
      "\n",
      "Answer from 'sy': As an AI, I can provide this information. A diffusion model is a conceptual framework or mathematical construct used to describe how something, often information, behaviors, or new ideas, spreads within a population. It's often used in various fields such as communications, marketing, public health, and more. The model typically involves a majority of individuals who are 'influenced' to adopt a new trend or idea by a small fraction of 'innovators' or 'early adopters'.\n",
      "\n",
      "1. Add Persona\n",
      "2. Query Persona\n",
      "3. Exit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5d06d3067817>\u001b[0m in \u001b[0;36m<cell line: 140>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# Step 3: Run chatbot interaction loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mchatbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_chatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-5d06d3067817>\u001b[0m in \u001b[0;36mrun_chatbot\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2. Query Persona\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Exit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Select an option: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PDFPlumberLoader,PDFMinerLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "import numpy as np\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class PersonaChatbot:\n",
    "    def __init__(self, openai_api_key):\n",
    "        self.general_vectordb = None\n",
    "        self.persona_vectordbs = {}\n",
    "        self.embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        self.chat_model = ChatOpenAI(model=\"gpt-4\", temperature=0.7, openai_api_key=openai_api_key)\n",
    "\n",
    "    def build_general_vectordb(self, pdf_paths):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=300,       # 각 청크의 최대 문자 수\n",
    "            chunk_overlap=50,     # 청크 간 중복 문자 수\n",
    "        )\n",
    "        \"\"\"Create a VectorDB for general AI knowledge from provided PDF files.\"\"\"\n",
    "        documents = []\n",
    "        for pdf_path in pdf_paths:\n",
    "            #loader = PyPDFLoader(pdf_path)\n",
    "            loader = PDFMinerLoader(pdf_path)\n",
    "            documents.extend(loader.load_and_split())\n",
    "        self.general_vectordb = FAISS.from_documents(documents, self.embedding_model)\n",
    "\n",
    "    def add_persona(self, persona_name, pdf_path):\n",
    "        \"\"\"Add a new persona with an associated PDF.\"\"\"\n",
    "        #loader = PyPDFLoader(pdf_path)\n",
    "        loader = PDFMinerLoader(pdf_path)\n",
    "        documents = loader.load_and_split()\n",
    "        persona_vectordb = FAISS.from_documents(documents, self.embedding_model)\n",
    "        self.persona_vectordbs[persona_name] = persona_vectordb\n",
    "\n",
    "    def cosine_similarity(self,vec1, vec2):\n",
    "        \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def query_persona(self, persona_name, query):\n",
    "        \"\"\"Query a specific persona and integrate with general knowledge.\"\"\"\n",
    "        if persona_name not in self.persona_vectordbs:\n",
    "            raise ValueError(f\"Persona '{persona_name}' does not exist.\")\n",
    "\n",
    "        persona_vectordb = self.persona_vectordbs[persona_name]\n",
    "\n",
    "        # Embed the query\n",
    "        query_embedding = self.embedding_model.embed_query(query)\n",
    "\n",
    "        # General knowledge retrieval\n",
    "        general_retriever = self.general_vectordb.as_retriever()\n",
    "        general_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.chat_model,\n",
    "            retriever=general_retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "\n",
    "        general_output = general_chain.invoke({\"query\": query})\n",
    "        general_result = general_output[\"result\"]\n",
    "\n",
    "        # Persona-specific retrieval\n",
    "        persona_retriever = persona_vectordb.as_retriever()\n",
    "        persona_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.chat_model,\n",
    "            retriever=persona_retriever,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        persona_output = persona_chain.invoke({\"query\": query})\n",
    "        persona_result = persona_output[\"result\"]\n",
    "\n",
    "        # Embed the results for similarity comparison\n",
    "        general_embedding = self.embedding_model.embed_query(general_result)\n",
    "        persona_embedding = self.embedding_model.embed_query(persona_result)\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        general_similarity = self.cosine_similarity(query_embedding, general_embedding)\n",
    "        persona_similarity = self.cosine_similarity(query_embedding, persona_embedding)\n",
    "        print('general_similarity:',general_similarity)\n",
    "        print('persona_similarity:',persona_similarity)\n",
    "        # Select the more relevant result\n",
    "        if general_similarity >= persona_similarity:\n",
    "            selected_result = general_result\n",
    "            source = \"General AI Knowledge\"\n",
    "            print(source)\n",
    "        else:\n",
    "            selected_result = persona_result\n",
    "            source = f\"Persona: {persona_name}\"\n",
    "            print(source)\n",
    "\n",
    "        # Generate the final prompt\n",
    "        combined_prompt = (\n",
    "            f\"You have access to the following source:\\n\\n\"\n",
    "            f\"{source}:\\n{selected_result}\\n\\n\"\n",
    "            f\"Using this information, answer the following query:\\n{query}\"\n",
    "        )\n",
    "        # Wrap the prompt in a HumanMessage\n",
    "        response = self.chat_model([HumanMessage(content=combined_prompt)])\n",
    "        return response.content\n",
    "\n",
    "\n",
    "    def run_chatbot(self):\n",
    "        \"\"\"Run the chatbot interaction loop.\"\"\"\n",
    "        print(\"Welcome to the Persona Chatbot Service!\")\n",
    "        while True:\n",
    "            print(\"\\n1. Add Persona\")\n",
    "            print(\"2. Query Persona\")\n",
    "            print(\"3. Exit\")\n",
    "            choice = input(\"Select an option: \")\n",
    "\n",
    "            if choice == \"1\":\n",
    "                persona_name = input(\"Enter Persona name: \")\n",
    "                pdf_path = input(\"Enter PDF path for the Persona (uploaded in Colab): \")\n",
    "                if os.path.exists(pdf_path):\n",
    "                    self.add_persona(persona_name, pdf_path)\n",
    "                    print(f\"Persona '{persona_name}' added successfully!\")\n",
    "                else:\n",
    "                    print(\"Invalid PDF path. Please try again.\")\n",
    "\n",
    "            elif choice == \"2\":\n",
    "                persona_name = input(\"Enter Persona name: \")\n",
    "                query = input(\"Enter your question: \")\n",
    "                try:\n",
    "                    answer = self.query_persona(persona_name, query)\n",
    "                    print(f\"\\nAnswer from '{persona_name}': {answer}\")\n",
    "                except ValueError as e:\n",
    "                    print(e)\n",
    "\n",
    "            elif choice == \"3\":\n",
    "                print(\"Exiting chatbot. Goodbye!\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "\n",
    "# Main function for Colab\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Provide OpenAI API Key\n",
    "    from getpass import getpass\n",
    "    #openai_api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "    openai_api_key='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'\n",
    "    # Initialize the chatbot\n",
    "    chatbot = PersonaChatbot(openai_api_key=openai_api_key)\n",
    "\n",
    "    # Step 2: Build general knowledge VectorDB\n",
    "    print(\"Provide paths to general knowledge PDFs (uploaded in Colab, comma-separated):\")\n",
    "    # pdf_paths = input().split(\",\")\n",
    "    pdf_directory = '/content/drive/MyDrive/ai papers/general'#2\n",
    "\n",
    "    valid_paths = [\n",
    "        os.path.join(pdf_directory, filename)\n",
    "        for filename in os.listdir(pdf_directory)\n",
    "        if filename.endswith('.pdf') and os.path.isfile(os.path.join(pdf_directory, filename))\n",
    "    ]\n",
    "    if valid_paths:\n",
    "        chatbot.build_general_vectordb(valid_paths)\n",
    "        print(\"General knowledge VectorDB and Knowledge Graph built successfully!\")\n",
    "    else:\n",
    "        print(\"No valid paths provided. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Step 3: Run chatbot interaction loop\n",
    "    chatbot.run_chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xVen4EG_WwH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
